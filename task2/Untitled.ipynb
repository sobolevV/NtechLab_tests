{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from glob import glob\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_data(base_dir, file_format=\".jpg\", valid_size=0.25):\n",
    "    dirs_inside = {}\n",
    "    for index, dir_name in enumerate(os.listdir(base_dir)):\n",
    "        if os.path.isdir(os.path.join(base_dir, dir_name)):\n",
    "            dirs_inside[dir_name] = index\n",
    "            \n",
    "    print(f'Found {len(dirs_inside.keys())} classes')\n",
    "    \n",
    "    train_data_x, train_data_y = [], []\n",
    "    valid_data_x, valid_data_y = [], []\n",
    "    \n",
    "    for sub_dir in dirs_inside.keys():\n",
    "        class_data = glob(os.path.join(base_dir, sub_dir) + f\"\\\\*{file_format}\", recursive=True) \n",
    "        class_samples_train = int(len(class_data) * (1 - valid_size))\n",
    "        print(f'Found {len(class_data)} images for {sub_dir}')\n",
    "        \n",
    "        train_data_x += class_data[:class_samples_train]    \n",
    "        valid_data_x += class_data[class_samples_train:]\n",
    "    \n",
    "    shuffle(train_data_x)\n",
    "    shuffle(valid_data_x)\n",
    "    train_data_y = [dirs_inside[file.split('\\\\')[1]] for file in train_data_x]\n",
    "    valid_data_y = [dirs_inside[file.split('\\\\')[1]] for file in valid_data_x] \n",
    "    \n",
    "    print(f\"Train size is {len(train_data_x)}, validation size is {len(valid_data_x)}\")\n",
    "    return train_data_x, train_data_y, valid_data_x, valid_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 classes\n",
      "Found 50001 images for female\n",
      "Found 50001 images for male\n",
      "Train size is 75000, validation size is 25002\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"internship_data\"\n",
    "train_x, train_y, valid_x, valid_y = get_train_valid_data(base_dir=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, files_list, base_dir, img_size=156, transform_compose=None):\n",
    "        self.files = files_list\n",
    "        self.classes = dict((cls, i) for i, cls in enumerate(os.listdir(base_dir)) if os.path.isdir(os.path.join(base_dir, cls)))\n",
    "        self.size = img_size\n",
    "        self.transforms = transforms.Compose([transforms.Resize((img_size, img_size)), \n",
    "                                              transforms.ToTensor(),]) if transform_compose is None else transform_compose  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        pillow_img = Image.open(self.files[key])\n",
    "        x = self.transforms(pillow_img)\n",
    "        y = [0. for i in range(len(self.classes))]\n",
    "        y[self.classes[self.files[key].split('\\\\')[1]]] = 1.\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CudaLoader:\n",
    "    \n",
    "    def __init__(self, DataLoader, shape):\n",
    "        self.dl = DataLoader\n",
    "        self.dev = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.img_shape = shape  # Must be (C, H, W)\n",
    "        print('Device is', self.dev)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iter_dl = iter(self.dl)\n",
    "        for xb, yb in iter_dl:\n",
    "            yield xb, yb  # self.preprocess(xb, yb)\n",
    "    \n",
    "    def preprocess(self, xb, yb):\n",
    "        return xb.view(-1, self.img_shape[0], self.img_shape[1], self.img_shape[2]).to(self.dev), yb.to(self.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 156\n",
    "train_ds = MyImageDataset(train_x, base_dir=data_folder, img_size=IMG_SIZE)\n",
    "valid_ds = MyImageDataset(valid_x, base_dir=data_folder, img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dl_train = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "tensor_dl_valid = DataLoader(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda:0\n",
      "Device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda_train = CudaLoader(tensor_dl_train, shape=(3, IMG_SIZE, IMG_SIZE))\n",
    "cuda_valid = CudaLoader(tensor_dl_valid, shape=(3, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def get_vgg_model(num_classes, feature_extracting=True):\n",
    "    \"\"\"Get saved vgg model with fixed parameters if feature_extracting = True\n",
    "        feature_extracting: bool\n",
    "    returns: model, input_size\"\"\"\n",
    "    input_size = 224\n",
    "    model = torch.load(\"vgg16_model.pt\")\n",
    "    set_parameter_requires_grad(model, feature_extracting=True)\n",
    "    \n",
    "    \n",
    "    num_features = model.classifier[6].in_features\n",
    "    features = list(model.classifier.children())[:-1] # Remove last layer\n",
    "    features.extend([nn.Linear(num_features, num_classes)]) # Add our layer with activation\n",
    "    model.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "    return model, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model, img_size = get_vgg_model(num_classes=2, feature_extracting=True)\n",
    "vgg_model.cuda()\n",
    "\n",
    "vgg_transforms = transforms.Compose([transforms.Resize((img_size, img_size)), \n",
    "                                     transforms.ToTensor(), \n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_ds_vgg = MyImageDataset(train_x, base_dir=data_folder, img_size=img_size, transform_compose=vgg_transforms)\n",
    "valid_ds_vgg = MyImageDataset(valid_x, base_dir=data_folder, img_size=img_size, transform_compose=vgg_transforms)\n",
    "\n",
    "vgg_dl_train = DataLoader(train_ds_vgg, batch_size=32, shuffle=True)\n",
    "vgg_dl_valid = DataLoader(valid_ds_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_vgg = torch.optim.Adam(vgg_model.parameters(), lr=1e-3)\n",
    "loss_function_vgg = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: internship_data\\female\\017263.jpg class [1.0, 0.0]\n",
      "file: internship_data\\male\\142672.jpg class [0.0, 1.0]\n",
      "file: internship_data\\female\\143205.jpg class [1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('file:', train_ds_vgg.files[i], 'class', train_ds_vgg[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(vgg_dl_train)\n",
    "xb, yb = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My simple Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels=3, output_size=1):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=428, kernel_size=3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(428*3*3, 1024)  # sizes from c5\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.out = nn.Linear(128, output_size)\n",
    "        \n",
    "        self.flatten_size = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c1 = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        c2 = F.max_pool2d(F.relu(self.conv2(c1)), (2, 2))\n",
    "        c3 = F.max_pool2d(F.relu(self.conv3(c2)), (2, 2))\n",
    "        c4 = F.max_pool2d(F.relu(self.conv4(c3)), (2, 2))\n",
    "        c5 = F.max_pool2d(F.relu(self.conv5(c4)), (2, 2))\n",
    "        \n",
    "        if self.flatten_size is None:\n",
    "            self.flatten_size = c5.shape[1] * c5.shape[2] * c5.shape[3]\n",
    "            print(c5.shape, self.flatten_size)\n",
    "            \n",
    "        c5 = c5.view(-1, self.flatten_size)\n",
    "        f1 = F.relu(self.fc1(c5))\n",
    "        f2 = F.relu(self.fc2(f1))\n",
    "        out = self.out(f2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Net()\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_tensor, target):\n",
    "    pred_classes = torch.argmax(pred_tensor, dim=1)\n",
    "#     true_classes = torch.argmax(target, dim=1)\n",
    "    equal_classes = (pred_classes == target).float()\n",
    "    return torch.mean(equal_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, loss_func, train_loader, valid_loader=None):\n",
    "    train_history = [[], []]\n",
    "    valid_history = [[], []]\n",
    "    for ep in range(epochs):\n",
    "        ep_train_loss, ep_train_acc = [], []\n",
    "        \n",
    "        for i, (xb, yb) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            yb = yb[1].long().cuda()\n",
    "            \n",
    "            preds = model(xb.cuda())\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = accuracy(preds, yb).item()\n",
    "            ep_train_acc.append(acc)\n",
    "            ep_train_loss.append(loss.item())\n",
    "            \n",
    "            print(acc, loss.item())\n",
    "            if i == 200:\n",
    "                break\n",
    "            \n",
    "        # mean accuracy and loss for current epoch\n",
    "        train_history[0].append(np.array(ep_train_acc).mean())\n",
    "        train_history[1].append(np.array(ep_train_loss).mean())\n",
    "        \n",
    "        if valid_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                eval_acc, eval_loss = [], []\n",
    "                for xb, yb in valid_loader:\n",
    "                    preds = model(xb.cuda())\n",
    "                    eval_loss.append(loss_func(preds, yb.float().cuda()).item())\n",
    "                    eval_acc.append(accuracy(preds, yb.float().cuda()).item())\n",
    "                # mean accuracy and loss for current epoch with validation data\n",
    "                valid_history[0].append(np.array(eval_acc).mean())\n",
    "                valid_history[1].append(np.array(eval_loss).mean())\n",
    "        \n",
    "        print(f'Epoch {ep}. acc:', round(np.array(ep_train_acc).mean(), 5), 'loss:', round(np.array(ep_train_loss).mean(), 5))\n",
    "    \n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875 0.2517974078655243\n",
      "0.96875 0.13388067483901978\n",
      "0.84375 0.4474097490310669\n",
      "0.96875 0.09961051493883133\n",
      "0.90625 0.6599254608154297\n",
      "0.75 0.7901031970977783\n",
      "0.9375 0.20271745324134827\n",
      "0.96875 0.05812651664018631\n",
      "0.96875 0.15721526741981506\n",
      "1.0 0.04517175629734993\n",
      "0.9375 0.4244026839733124\n",
      "0.9375 0.23847399652004242\n",
      "0.96875 0.05725773796439171\n",
      "0.9375 0.1249626874923706\n",
      "0.90625 0.20293526351451874\n",
      "0.96875 0.10476718097925186\n",
      "0.875 0.18727894127368927\n",
      "0.84375 0.32219773530960083\n",
      "0.9375 0.2590535283088684\n",
      "0.9375 0.41855326294898987\n",
      "0.90625 0.2780841290950775\n",
      "0.90625 0.21175704896450043\n",
      "0.90625 0.13331152498722076\n",
      "0.875 0.5316754579544067\n",
      "0.96875 0.08402818441390991\n",
      "0.9375 0.0998460203409195\n",
      "0.875 0.32924968004226685\n",
      "0.875 0.3359799087047577\n",
      "0.9375 0.36226704716682434\n",
      "0.84375 0.47854018211364746\n",
      "0.96875 0.07070047408342361\n",
      "0.9375 0.1947792023420334\n",
      "0.875 0.35964369773864746\n",
      "0.96875 0.40433380007743835\n",
      "0.90625 0.1947077512741089\n",
      "0.8125 0.6496139764785767\n",
      "0.875 0.4595957100391388\n",
      "0.90625 0.20453651249408722\n",
      "0.875 0.33903491497039795\n",
      "0.90625 0.21642592549324036\n",
      "0.9375 0.16250622272491455\n",
      "0.9375 0.14913791418075562\n",
      "0.90625 0.3790892958641052\n",
      "0.9375 0.23350073397159576\n",
      "0.90625 0.48146727681159973\n",
      "0.875 0.45158812403678894\n",
      "0.78125 0.8224325180053711\n",
      "0.96875 0.09997928887605667\n",
      "0.9375 0.16528372466564178\n",
      "0.78125 0.5428296327590942\n",
      "0.84375 0.5323595404624939\n",
      "0.875 0.652511715888977\n",
      "0.90625 0.24191153049468994\n",
      "0.875 0.15699495375156403\n",
      "0.96875 0.19125017523765564\n",
      "0.9375 0.09364394843578339\n",
      "0.9375 0.10587067157030106\n",
      "0.96875 0.1107204481959343\n",
      "0.9375 0.17558375000953674\n",
      "0.9375 0.09588620066642761\n",
      "0.9375 0.10113581269979477\n",
      "0.90625 0.45242226123809814\n",
      "0.9375 0.4012949466705322\n",
      "0.75 1.7333714962005615\n",
      "0.90625 0.7248044610023499\n",
      "0.84375 0.5718194246292114\n",
      "0.875 0.5176925659179688\n",
      "0.9375 0.1877947300672531\n",
      "0.90625 0.2699735164642334\n",
      "0.9375 0.2586180567741394\n",
      "0.875 0.32928702235221863\n",
      "0.9375 0.22771742939949036\n",
      "0.90625 0.3478100895881653\n",
      "0.84375 1.145007848739624\n",
      "0.84375 0.3190154731273651\n",
      "0.90625 0.21330659091472626\n",
      "0.78125 0.673550009727478\n",
      "0.71875 0.9691820740699768\n",
      "0.875 0.3708253502845764\n",
      "0.9375 0.18497709929943085\n",
      "0.875 0.873647928237915\n",
      "0.78125 0.7507071495056152\n",
      "0.875 0.3596479296684265\n",
      "0.84375 0.35778266191482544\n",
      "0.90625 0.3811465799808502\n",
      "0.96875 0.04114982858300209\n",
      "0.9375 0.22426599264144897\n",
      "0.9375 0.34195077419281006\n",
      "0.875 0.452140748500824\n",
      "0.90625 0.5146874785423279\n",
      "0.90625 0.8088720440864563\n",
      "0.875 0.5030065178871155\n",
      "0.90625 0.3088171184062958\n",
      "0.90625 0.16802003979682922\n",
      "0.9375 0.2503054440021515\n",
      "0.8125 0.47286146879196167\n",
      "1.0 0.05624016746878624\n",
      "0.90625 0.35579150915145874\n",
      "0.875 0.22836264967918396\n",
      "1.0 0.026778120547533035\n",
      "0.9375 0.272574782371521\n",
      "0.78125 0.7884842753410339\n",
      "0.75 0.571590781211853\n",
      "0.9375 0.129571795463562\n",
      "0.8125 0.49115923047065735\n",
      "0.90625 0.4033103287220001\n",
      "0.84375 0.256230890750885\n",
      "0.90625 0.5031999349594116\n",
      "0.96875 0.12168702483177185\n",
      "0.9375 0.14920002222061157\n",
      "0.90625 0.16153950989246368\n",
      "0.96875 0.1057988852262497\n",
      "0.9375 0.56694096326828\n",
      "0.96875 0.06220480427145958\n",
      "0.84375 0.5198091864585876\n",
      "0.96875 0.1540629267692566\n",
      "0.84375 0.30298489332199097\n",
      "0.90625 0.3949008584022522\n",
      "0.96875 0.0840243548154831\n",
      "0.90625 0.4301213324069977\n",
      "0.96875 0.12437579780817032\n",
      "0.875 0.4468555152416229\n",
      "0.90625 0.27000194787979126\n",
      "0.90625 0.2738892138004303\n",
      "1.0 0.02362923137843609\n",
      "0.9375 0.24827709794044495\n",
      "0.875 0.6620073914527893\n",
      "0.875 0.9936457872390747\n",
      "0.96875 0.08886216580867767\n",
      "0.96875 0.11257319897413254\n",
      "0.90625 0.17052677273750305\n",
      "0.96875 0.12734338641166687\n",
      "0.96875 0.09003452956676483\n",
      "1.0 0.058729592710733414\n",
      "0.90625 0.30162158608436584\n",
      "0.90625 0.8135833740234375\n",
      "0.875 0.41622939705848694\n",
      "0.84375 0.3160490393638611\n",
      "0.9375 0.092388816177845\n",
      "0.96875 0.10434252768754959\n",
      "0.875 0.19663570821285248\n",
      "0.84375 0.40498530864715576\n",
      "0.90625 0.2761251926422119\n",
      "0.90625 0.23070868849754333\n",
      "0.90625 0.27871865034103394\n",
      "0.96875 0.10331984609365463\n",
      "0.9375 0.19462087750434875\n",
      "0.84375 0.3966510593891144\n",
      "0.96875 0.14218397438526154\n",
      "1.0 0.05542875826358795\n",
      "0.9375 0.12573958933353424\n",
      "0.96875 0.15798090398311615\n",
      "0.9375 0.08419445902109146\n",
      "0.9375 0.5169455409049988\n",
      "0.875 0.5832663774490356\n",
      "0.875 0.6830082535743713\n",
      "0.75 0.7409704923629761\n",
      "0.96875 0.4115489423274994\n",
      "0.9375 0.18375541269779205\n",
      "0.9375 0.09871849417686462\n",
      "0.90625 0.15631064772605896\n",
      "0.96875 0.1596156656742096\n",
      "0.9375 0.1716419756412506\n",
      "0.84375 0.30783307552337646\n",
      "0.9375 0.15270330011844635\n",
      "0.96875 0.14116479456424713\n",
      "0.90625 0.14275187253952026\n",
      "0.9375 0.2243822067975998\n",
      "0.90625 0.4552251696586609\n",
      "0.875 0.15499812364578247\n",
      "0.8125 0.7719688415527344\n",
      "0.90625 0.33277592062950134\n",
      "0.875 0.3010653257369995\n",
      "0.84375 0.5014733672142029\n",
      "0.78125 0.46437960863113403\n",
      "0.9375 0.18496273458003998\n",
      "0.90625 0.19319765269756317\n",
      "0.9375 0.13120628893375397\n",
      "0.8125 0.4381629228591919\n",
      "0.90625 0.24288241565227509\n",
      "0.78125 0.6515923142433167\n",
      "0.96875 0.11084862798452377\n",
      "0.875 0.2555171251296997\n",
      "0.96875 0.08362261950969696\n",
      "0.875 0.2883024215698242\n",
      "0.9375 0.22992029786109924\n",
      "0.875 0.4103367328643799\n",
      "0.875 0.35409682989120483\n",
      "0.9375 0.5931066870689392\n",
      "1.0 0.04286860302090645\n",
      "0.96875 0.07705879956483841\n",
      "0.96875 0.26951655745506287\n",
      "0.90625 0.204277902841568\n",
      "0.90625 0.5149204134941101\n",
      "0.90625 0.2147676646709442\n",
      "0.84375 0.5643666386604309\n",
      "0.875 0.3546825349330902\n",
      "0.9375 0.19811159372329712\n",
      "0.875 0.5344514846801758\n",
      "0.8125 0.5633873343467712\n",
      "0.90625 0.7351672053337097\n",
      "Epoch 0. acc: 0.90547 loss: 0.32534\n"
     ]
    }
   ],
   "source": [
    "train_hist, valid_hist = train(vgg_model, epochs=1, optimizer=optimizer_vgg, loss_func=loss_function_vgg, train_loader=vgg_dl_train, valid_loader=None) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, data_loader, num_samples=100, cuda_model=True):\n",
    "    predictions, real_classes = [], []\n",
    "    for sample_index, (xb, yb) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            if cuda_model:\n",
    "                preds = vgg_model(xb.cuda())\n",
    "            else:\n",
    "                preds = vgg_model(xb)\n",
    "                \n",
    "        predictions.append(torch.argmax(preds.cpu()).item())\n",
    "        real_classes.append(yb[1].item())\n",
    "        if sample_index == num_samples:\n",
    "            break\n",
    "            \n",
    "    predictions = np.array(predictions, dtype=int)\n",
    "    real_classes = np.array(real_classes, dtype=int)\n",
    "    print(\"accuracy is\", np.mean((predictions==real_classes).astype(float)) )\n",
    "    return predictions, real_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.9340659340659341\n"
     ]
    }
   ],
   "source": [
    "preds, real = validate_model(vgg_model, vgg_dl_valid, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictid class 1.0, real class 1\n",
      "Predictid class 1.0, real class 1\n",
      "Predictid class 0.0, real class 0\n"
     ]
    }
   ],
   "source": [
    "for i, (xb, yb) in enumerate(vgg_dl_valid):\n",
    "    with torch.no_grad():\n",
    "        preds = vgg_model(xb.cuda())\n",
    "    preds = torch.round(preds)\n",
    "    print(f'Predictid class {preds.squeeze().item()}, real class {yb.item()}')\n",
    "    if i == 2:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env] *",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
