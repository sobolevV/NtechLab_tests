{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from glob import glob\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_data(base_dir, file_format=\".jpg\", valid_size=0.25):\n",
    "    dirs_inside = {}  # class - index dict\n",
    "    for index, dir_name in enumerate(os.listdir(base_dir)):\n",
    "        # add only directories inside\n",
    "        if os.path.isdir(os.path.join(base_dir, dir_name)):\n",
    "            dirs_inside[dir_name] = float(index)\n",
    "            \n",
    "    print(f'Found {len(dirs_inside.keys())} classes')\n",
    "    # create train and validate lists for images paths\n",
    "    train_data_x, train_data_y = [], []\n",
    "    valid_data_x, valid_data_y = [], []\n",
    "    \n",
    "    for sub_dir in dirs_inside.keys():\n",
    "        # get all images inside directory\n",
    "        class_data = glob(os.path.join(base_dir, sub_dir) + f\"\\\\*{file_format}\", recursive=True) \n",
    "        # get count samples for training set\n",
    "        class_samples_train = int(len(class_data) * (1 - valid_size))\n",
    "        print(f'Found {len(class_data)} images for {sub_dir}')\n",
    "        \n",
    "        train_data_x += class_data[:class_samples_train]    \n",
    "        valid_data_x += class_data[class_samples_train:]\n",
    "    # shuffle data\n",
    "    shuffle(train_data_x)\n",
    "    shuffle(valid_data_x)\n",
    "    # set labels\n",
    "    train_data_y = [dirs_inside[file.split('\\\\')[1]] for file in train_data_x]\n",
    "    valid_data_y = [dirs_inside[file.split('\\\\')[1]] for file in valid_data_x] \n",
    "    \n",
    "    print(f\"Train size is {len(train_data_x)}, validation size is {len(valid_data_x)}\")\n",
    "    return train_data_x, train_data_y, valid_data_x, valid_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 classes\n",
      "Found 50001 images for female\n",
      "Found 50001 images for male\n",
      "Train size is 75000, validation size is 25002\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"internship_data\"\n",
    "train_x, train_y, valid_x, valid_y = get_train_valid_data(base_dir=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImageDataset(Dataset):\n",
    "    \"\"\"Create pytorch iterable Dataset to generate images with X paths\n",
    "    params:\n",
    "        X[list] - list of paths for images\n",
    "        Y[list] - list of labels for X\n",
    "        img_size (int) - size of output images. Default 156\n",
    "        transform_compose (torchvision.transforms.Compose) - transforms for output images. \n",
    "            Default: transforms.Compose([transforms.Resize((img_size, img_size)), transforms.ToTensor(),])\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, base_dir, img_size=156, transform_compose=None):\n",
    "        self.files = X\n",
    "        self.classes = Y\n",
    "        self.size = img_size\n",
    "        self.transforms = transforms.Compose([transforms.Resize((img_size, img_size)), \n",
    "                                              transforms.ToTensor(),]) if transform_compose is None else transform_compose  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        # get image by path with index=key\n",
    "        pillow_img = Image.open(self.files[key])\n",
    "        # apply transforms to image\n",
    "        x = self.transforms(pillow_img)\n",
    "        y = self.classes[key]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, data_loader, num_samples=100, cuda_model=True):\n",
    "    predictions, real_classes = [], []\n",
    "    for sample_index, (xb, yb) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            if cuda_model:\n",
    "                preds = vgg_model(xb.cuda())\n",
    "            else:\n",
    "                preds = vgg_model(xb)\n",
    "                \n",
    "        predictions.append(torch.argmax(preds.cpu()).item())\n",
    "        real_classes.append(yb.item())\n",
    "        if sample_index == num_samples:\n",
    "            break\n",
    "            \n",
    "    predictions = np.array(predictions, dtype=int)\n",
    "    real_classes = np.array(real_classes, dtype=int)\n",
    "    print(\"accuracy is\", np.mean((predictions==real_classes).astype(float)) )\n",
    "    return predictions, real_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    \"\"\"Seting model parameters not trainable\"\"\"\n",
    "    if feature_extracting:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def get_vgg_model(num_classes, path='vgg16_model.pt', feature_extracting=True):\n",
    "    \"\"\"Get vgg model with fixed parameters if feature_extracting = True.\n",
    "    params:\n",
    "        num_classes (int): count of outputs for model\n",
    "        feature_extracting (bool): sets models parameters not trainable\n",
    "    returns: model, input_image_size\n",
    "    \"\"\"\n",
    "    input_size = 224  # input size for vgg pretrained model\n",
    "    model = torch.load(path)  # my default path\n",
    "    set_parameter_requires_grad(model, feature_extracting=True)\n",
    "    \n",
    "    num_features = model.classifier[6].in_features  # size of input for 6-nd fc layer\n",
    "    features = list(model.classifier.children())[:-1] # remove last layer\n",
    "    features.extend([nn.Linear(num_features, num_classes)]) # add new layer with new num_classes\n",
    "    model.classifier = nn.Sequential(*features) # replace the model classifier\n",
    "\n",
    "    return model, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "vgg_model, img_size = get_vgg_model(num_classes=2, feature_extracting=True)\n",
    "# load to gpu\n",
    "vgg_model.cuda()\n",
    "optimizer_vgg = torch.optim.Adam(vgg_model.parameters(), lr=1e-3)\n",
    "loss_function_vgg = nn.CrossEntropyLoss()  # classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use standart transforms with normalization and without augmentation\n",
    "vgg_transforms = transforms.Compose([transforms.Resize((img_size, img_size)), \n",
    "                                     transforms.ToTensor(), \n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "# create dataset\n",
    "train_ds_vgg = MyImageDataset(train_x, train_y, base_dir=data_folder, img_size=img_size, transform_compose=vgg_transforms)\n",
    "valid_ds_vgg = MyImageDataset(valid_x, valid_y, base_dir=data_folder, img_size=img_size, transform_compose=vgg_transforms)\n",
    "# create pytorch batch loaders\n",
    "vgg_dl_train = DataLoader(train_ds_vgg, batch_size=32, shuffle=True, drop_last=True)\n",
    "vgg_dl_valid = DataLoader(valid_ds_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels=3, output_size=1):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=428, kernel_size=3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(428*3*3, 1024)  # sizes from c5\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.out = nn.Linear(128, output_size)\n",
    "        \n",
    "        self.flatten_size = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c1 = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        c2 = F.max_pool2d(F.relu(self.conv2(c1)), (2, 2))\n",
    "        c3 = F.max_pool2d(F.relu(self.conv3(c2)), (2, 2))\n",
    "        c4 = F.max_pool2d(F.relu(self.conv4(c3)), (2, 2))\n",
    "        c5 = F.max_pool2d(F.relu(self.conv5(c4)), (2, 2))\n",
    "        \n",
    "        if self.flatten_size is None:\n",
    "            self.flatten_size = c5.shape[1] * c5.shape[2] * c5.shape[3]\n",
    "            \n",
    "        c5 = c5.view(-1, self.flatten_size)\n",
    "        f1 = F.relu(self.fc1(c5))\n",
    "        f2 = F.relu(self.fc2(f1))\n",
    "        out = self.out(f2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Net(output_size=2)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 156\n",
    "train_ds = MyImageDataset(train_x, train_y, base_dir=data_folder, img_size=IMG_SIZE)\n",
    "valid_ds = MyImageDataset(valid_x, valid_y, base_dir=data_folder, img_size=IMG_SIZE)\n",
    "\n",
    "tensor_dl_train = DataLoader(train_ds, batch_size=32, shuffle=True, pin_memory=False)\n",
    "tensor_dl_valid = DataLoader(valid_ds, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_tensor, target):\n",
    "    pred_classes = torch.argmax(pred_tensor, dim=1)\n",
    "    equal_classes = (pred_classes == target).float()\n",
    "    return torch.mean(equal_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, loss_func, train_loader, valid_loader=None, train_frac=1.):\n",
    "    # create history for each epoch list for metrics\n",
    "    train_history = [[], []]\n",
    "    valid_history = [[], []]\n",
    "    for ep in range(epochs):\n",
    "        # history for current epoch\n",
    "        ep_train_loss, ep_train_acc = [], []\n",
    "        for i, (xb, yb) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # pass to gpu\n",
    "            yb = yb.long().cuda()\n",
    "            # get predictions\n",
    "            preds = model(xb.cuda())\n",
    "            # calculate loss\n",
    "            loss = loss_func(preds, yb)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate metrics for batch\n",
    "            acc = accuracy(preds, yb).item()\n",
    "            ep_train_acc.append(acc)\n",
    "            ep_train_loss.append(loss.item())\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Batch {i+1}. Accuracy={acc:.5}, Loss={loss.item()}')\n",
    "            \n",
    "            if i / len(train_loader) >= train_frac:\n",
    "                break\n",
    "            \n",
    "        # mean accuracy and loss for current epoch\n",
    "        train_history[0].append(np.array(ep_train_acc).mean())\n",
    "        train_history[1].append(np.array(ep_train_loss).mean())\n",
    "        \n",
    "        if valid_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                eval_acc, eval_loss = [], []\n",
    "                for xb, yb in valid_loader:\n",
    "                    preds = model(xb.cuda())\n",
    "                    eval_loss.append(loss_func(preds, yb.float().cuda()).item())\n",
    "                    eval_acc.append(accuracy(preds, yb.float().cuda()).item())\n",
    "                # mean accuracy and loss for current epoch with validation data\n",
    "                valid_history[0].append(np.array(eval_acc).mean())\n",
    "                valid_history[1].append(np.array(eval_loss).mean())\n",
    "        \n",
    "        print(f'Epoch {ep}. acc:', round(np.array(ep_train_acc).mean(), 5), 'loss:', round(np.array(ep_train_loss).mean(), 5))\n",
    "    \n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Train model VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1. Accuracy=0.34375, Loss=0.778610110282898\n",
      "Batch 11. Accuracy=0.875, Loss=0.42155393958091736\n",
      "Batch 21. Accuracy=0.75, Loss=1.410356044769287\n",
      "Batch 31. Accuracy=0.78125, Loss=0.9105362296104431\n",
      "Batch 41. Accuracy=0.9375, Loss=0.3288538157939911\n",
      "Batch 51. Accuracy=0.90625, Loss=0.22524712979793549\n",
      "Batch 61. Accuracy=0.875, Loss=0.2616981863975525\n",
      "Batch 71. Accuracy=0.96875, Loss=0.19793248176574707\n",
      "Batch 81. Accuracy=0.875, Loss=0.3303065299987793\n",
      "Batch 91. Accuracy=0.90625, Loss=0.23510144650936127\n",
      "Batch 101. Accuracy=1.0, Loss=0.05649895220994949\n",
      "Batch 111. Accuracy=0.96875, Loss=0.16274334490299225\n",
      "Batch 121. Accuracy=0.875, Loss=0.2142927050590515\n",
      "Batch 131. Accuracy=0.875, Loss=0.31840959191322327\n",
      "Batch 141. Accuracy=0.8125, Loss=0.5947136282920837\n",
      "Batch 151. Accuracy=0.9375, Loss=0.2503553032875061\n",
      "Batch 161. Accuracy=0.84375, Loss=0.4410962760448456\n",
      "Batch 171. Accuracy=0.96875, Loss=0.12755605578422546\n",
      "Batch 181. Accuracy=0.71875, Loss=0.6765922904014587\n",
      "Batch 191. Accuracy=1.0, Loss=0.049811217933893204\n",
      "Batch 201. Accuracy=0.90625, Loss=0.4378753900527954\n",
      "Batch 211. Accuracy=0.9375, Loss=0.1947917342185974\n",
      "Batch 221. Accuracy=0.96875, Loss=0.11660054326057434\n",
      "Batch 231. Accuracy=0.90625, Loss=0.19944894313812256\n",
      "Batch 241. Accuracy=0.8125, Loss=0.6387133598327637\n",
      "Batch 251. Accuracy=0.90625, Loss=0.14917151629924774\n",
      "Batch 261. Accuracy=0.875, Loss=0.3938032388687134\n",
      "Batch 271. Accuracy=0.90625, Loss=0.36228466033935547\n",
      "Batch 281. Accuracy=0.875, Loss=0.33215847611427307\n",
      "Batch 291. Accuracy=0.78125, Loss=0.6086264252662659\n",
      "Batch 301. Accuracy=0.875, Loss=0.17434564232826233\n",
      "Batch 311. Accuracy=0.875, Loss=0.3211641311645508\n",
      "Batch 321. Accuracy=0.90625, Loss=0.4115977883338928\n",
      "Batch 331. Accuracy=0.90625, Loss=0.2635030448436737\n",
      "Batch 341. Accuracy=0.9375, Loss=0.11274933815002441\n",
      "Batch 351. Accuracy=0.9375, Loss=0.18405185639858246\n",
      "Batch 361. Accuracy=1.0, Loss=0.01739472895860672\n",
      "Batch 371. Accuracy=0.90625, Loss=0.562961757183075\n",
      "Batch 381. Accuracy=0.9375, Loss=0.16454926133155823\n",
      "Batch 391. Accuracy=0.9375, Loss=0.09784742444753647\n",
      "Batch 401. Accuracy=0.875, Loss=0.2390918880701065\n",
      "Batch 411. Accuracy=0.90625, Loss=0.1453867405653\n",
      "Batch 421. Accuracy=1.0, Loss=0.05333155393600464\n",
      "Batch 431. Accuracy=0.90625, Loss=0.2883990406990051\n",
      "Batch 441. Accuracy=0.96875, Loss=0.15570266544818878\n",
      "Batch 451. Accuracy=0.8125, Loss=0.509033739566803\n",
      "Batch 461. Accuracy=0.875, Loss=0.42844951152801514\n",
      "Batch 471. Accuracy=0.78125, Loss=0.45347899198532104\n",
      "Batch 481. Accuracy=0.90625, Loss=0.20768336951732635\n",
      "Batch 491. Accuracy=0.84375, Loss=0.38624757528305054\n",
      "Batch 501. Accuracy=0.9375, Loss=0.18378956615924835\n",
      "Batch 511. Accuracy=0.96875, Loss=0.08346560597419739\n",
      "Batch 521. Accuracy=0.90625, Loss=0.1627701073884964\n",
      "Batch 531. Accuracy=0.90625, Loss=0.17311598360538483\n",
      "Batch 541. Accuracy=0.90625, Loss=0.43991559743881226\n",
      "Batch 551. Accuracy=0.90625, Loss=0.5942475199699402\n",
      "Batch 561. Accuracy=0.90625, Loss=0.1746518462896347\n",
      "Batch 571. Accuracy=0.9375, Loss=0.16104264557361603\n",
      "Batch 581. Accuracy=0.90625, Loss=0.17411743104457855\n",
      "Batch 591. Accuracy=0.84375, Loss=0.6316757798194885\n",
      "Batch 601. Accuracy=0.84375, Loss=0.8843731880187988\n",
      "Batch 611. Accuracy=0.875, Loss=0.6514748334884644\n",
      "Batch 621. Accuracy=1.0, Loss=0.024957606568932533\n",
      "Batch 631. Accuracy=0.96875, Loss=0.051792073994874954\n",
      "Batch 641. Accuracy=0.9375, Loss=0.17268303036689758\n",
      "Batch 651. Accuracy=0.90625, Loss=0.21610714495182037\n",
      "Batch 661. Accuracy=0.96875, Loss=0.08831318467855453\n",
      "Batch 671. Accuracy=0.90625, Loss=0.3650933504104614\n",
      "Batch 681. Accuracy=0.875, Loss=0.4670301675796509\n",
      "Batch 691. Accuracy=0.90625, Loss=0.21159394085407257\n",
      "Batch 701. Accuracy=1.0, Loss=0.056833427399396896\n",
      "Batch 711. Accuracy=0.96875, Loss=0.07990021258592606\n",
      "Batch 721. Accuracy=1.0, Loss=0.032516803592443466\n",
      "Batch 731. Accuracy=0.90625, Loss=0.178265780210495\n",
      "Batch 741. Accuracy=0.84375, Loss=0.5147998929023743\n",
      "Batch 751. Accuracy=0.90625, Loss=0.24997912347316742\n",
      "Batch 761. Accuracy=0.875, Loss=0.1939130425453186\n",
      "Batch 771. Accuracy=0.90625, Loss=0.22497200965881348\n",
      "Batch 781. Accuracy=0.875, Loss=0.37153154611587524\n",
      "Batch 791. Accuracy=0.875, Loss=0.6625927686691284\n",
      "Batch 801. Accuracy=0.90625, Loss=0.12699951231479645\n",
      "Batch 811. Accuracy=0.9375, Loss=0.14870406687259674\n",
      "Batch 821. Accuracy=0.96875, Loss=0.11063389480113983\n",
      "Batch 831. Accuracy=0.96875, Loss=0.05424113944172859\n",
      "Batch 841. Accuracy=0.875, Loss=0.2411780208349228\n",
      "Batch 851. Accuracy=0.96875, Loss=0.05640832334756851\n",
      "Batch 861. Accuracy=0.875, Loss=0.26426491141319275\n",
      "Batch 871. Accuracy=0.90625, Loss=0.2580347955226898\n",
      "Batch 881. Accuracy=0.875, Loss=0.28499138355255127\n",
      "Batch 891. Accuracy=0.90625, Loss=0.21151044964790344\n",
      "Batch 901. Accuracy=0.84375, Loss=0.2822946310043335\n",
      "Batch 911. Accuracy=0.875, Loss=0.422657310962677\n",
      "Batch 921. Accuracy=0.9375, Loss=0.230804443359375\n",
      "Batch 931. Accuracy=0.75, Loss=0.437046617269516\n",
      "Batch 941. Accuracy=0.9375, Loss=0.19697469472885132\n",
      "Batch 951. Accuracy=0.90625, Loss=0.4385349154472351\n",
      "Batch 961. Accuracy=0.90625, Loss=0.1854449361562729\n",
      "Batch 971. Accuracy=0.90625, Loss=0.13890810310840607\n",
      "Batch 981. Accuracy=0.9375, Loss=0.31277740001678467\n",
      "Batch 991. Accuracy=0.96875, Loss=0.05892445519566536\n",
      "Batch 1001. Accuracy=0.9375, Loss=0.25948411226272583\n",
      "Batch 1011. Accuracy=0.96875, Loss=0.7943246364593506\n",
      "Batch 1021. Accuracy=0.96875, Loss=0.11366263777017593\n",
      "Batch 1031. Accuracy=0.96875, Loss=0.10964925587177277\n",
      "Batch 1041. Accuracy=0.90625, Loss=0.17355896532535553\n",
      "Batch 1051. Accuracy=0.9375, Loss=0.12282083928585052\n",
      "Batch 1061. Accuracy=0.875, Loss=0.26598724722862244\n",
      "Batch 1071. Accuracy=1.0, Loss=0.022205781191587448\n",
      "Batch 1081. Accuracy=0.90625, Loss=0.8506695628166199\n",
      "Batch 1091. Accuracy=0.8125, Loss=0.5427690744400024\n",
      "Batch 1101. Accuracy=0.9375, Loss=0.172885924577713\n",
      "Batch 1111. Accuracy=0.84375, Loss=0.31049832701683044\n",
      "Batch 1121. Accuracy=0.96875, Loss=0.10580939054489136\n",
      "Batch 1131. Accuracy=0.9375, Loss=0.16026125848293304\n",
      "Batch 1141. Accuracy=0.9375, Loss=0.10702825337648392\n",
      "Batch 1151. Accuracy=0.8125, Loss=0.3501221835613251\n",
      "Batch 1161. Accuracy=0.9375, Loss=0.20056433975696564\n",
      "Batch 1171. Accuracy=0.9375, Loss=0.18371234834194183\n",
      "Epoch 0. acc: 0.90574 loss: 0.31068\n",
      "Batch 1. Accuracy=0.90625, Loss=0.5051748156547546\n",
      "Batch 11. Accuracy=0.9375, Loss=0.2553866505622864\n",
      "Batch 21. Accuracy=0.9375, Loss=0.2226772904396057\n",
      "Batch 31. Accuracy=0.9375, Loss=0.10476361960172653\n",
      "Batch 41. Accuracy=0.8125, Loss=0.727628231048584\n",
      "Batch 51. Accuracy=0.90625, Loss=0.5213727951049805\n",
      "Batch 61. Accuracy=1.0, Loss=0.11462301760911942\n",
      "Batch 71. Accuracy=0.9375, Loss=0.1313779652118683\n",
      "Batch 81. Accuracy=0.9375, Loss=0.26541826128959656\n",
      "Batch 91. Accuracy=0.90625, Loss=0.10220123827457428\n",
      "Batch 101. Accuracy=0.9375, Loss=0.1929442137479782\n",
      "Batch 111. Accuracy=0.90625, Loss=0.29122909903526306\n",
      "Batch 121. Accuracy=0.90625, Loss=0.15295067429542542\n",
      "Batch 131. Accuracy=0.96875, Loss=0.04448152333498001\n",
      "Batch 141. Accuracy=0.9375, Loss=0.145755335688591\n",
      "Batch 151. Accuracy=0.96875, Loss=0.07914342731237411\n",
      "Batch 161. Accuracy=0.9375, Loss=0.12510572373867035\n",
      "Batch 171. Accuracy=0.90625, Loss=0.47910237312316895\n",
      "Batch 181. Accuracy=0.9375, Loss=0.07453496009111404\n",
      "Batch 191. Accuracy=0.9375, Loss=0.3516566753387451\n",
      "Batch 201. Accuracy=1.0, Loss=0.046682778745889664\n",
      "Batch 211. Accuracy=0.9375, Loss=0.4203052818775177\n",
      "Batch 221. Accuracy=0.8125, Loss=0.47374752163887024\n",
      "Batch 231. Accuracy=0.90625, Loss=0.3669126033782959\n",
      "Batch 241. Accuracy=0.9375, Loss=0.10515813529491425\n",
      "Batch 251. Accuracy=1.0, Loss=0.0606444776058197\n",
      "Batch 261. Accuracy=0.96875, Loss=0.0874687135219574\n",
      "Batch 271. Accuracy=0.9375, Loss=0.1449824571609497\n",
      "Batch 281. Accuracy=0.90625, Loss=0.17462769150733948\n",
      "Batch 291. Accuracy=0.9375, Loss=0.19501398503780365\n",
      "Batch 301. Accuracy=0.96875, Loss=0.079731285572052\n",
      "Batch 311. Accuracy=0.90625, Loss=0.29748639464378357\n",
      "Batch 321. Accuracy=0.84375, Loss=0.36454862356185913\n",
      "Batch 331. Accuracy=0.9375, Loss=0.3136734962463379\n",
      "Batch 341. Accuracy=0.90625, Loss=0.25046306848526\n",
      "Batch 351. Accuracy=0.9375, Loss=0.15710961818695068\n",
      "Batch 361. Accuracy=0.96875, Loss=0.03630886971950531\n",
      "Batch 371. Accuracy=0.9375, Loss=0.0810299813747406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 381. Accuracy=0.9375, Loss=0.2713235020637512\n",
      "Batch 391. Accuracy=0.96875, Loss=0.5432876348495483\n",
      "Batch 401. Accuracy=0.875, Loss=0.22043998539447784\n",
      "Batch 411. Accuracy=1.0, Loss=0.09387548267841339\n",
      "Batch 421. Accuracy=0.9375, Loss=0.2902231514453888\n",
      "Batch 431. Accuracy=0.9375, Loss=0.24864014983177185\n",
      "Batch 441. Accuracy=0.9375, Loss=0.12991860508918762\n",
      "Batch 451. Accuracy=0.84375, Loss=0.245723158121109\n",
      "Batch 461. Accuracy=0.9375, Loss=0.19979076087474823\n",
      "Batch 471. Accuracy=0.9375, Loss=0.12286727130413055\n",
      "Batch 481. Accuracy=0.9375, Loss=0.357485294342041\n",
      "Batch 491. Accuracy=0.90625, Loss=0.3562089800834656\n",
      "Batch 501. Accuracy=0.90625, Loss=0.1700412482023239\n",
      "Batch 511. Accuracy=0.90625, Loss=0.3442966341972351\n",
      "Batch 521. Accuracy=0.9375, Loss=0.1900239884853363\n",
      "Batch 531. Accuracy=0.96875, Loss=0.11628502607345581\n",
      "Batch 541. Accuracy=0.96875, Loss=0.08531434088945389\n",
      "Batch 551. Accuracy=0.9375, Loss=0.07039805501699448\n",
      "Batch 561. Accuracy=0.875, Loss=0.14452996850013733\n",
      "Batch 571. Accuracy=0.96875, Loss=0.0627780482172966\n",
      "Batch 581. Accuracy=0.90625, Loss=0.41271770000457764\n",
      "Batch 591. Accuracy=0.90625, Loss=0.14188772439956665\n",
      "Batch 601. Accuracy=0.9375, Loss=0.4063519835472107\n",
      "Batch 611. Accuracy=0.90625, Loss=0.1660025417804718\n",
      "Batch 621. Accuracy=0.875, Loss=0.3341100215911865\n",
      "Batch 631. Accuracy=0.96875, Loss=0.07815759629011154\n",
      "Batch 641. Accuracy=0.90625, Loss=0.19873543083667755\n",
      "Batch 651. Accuracy=0.9375, Loss=0.21266824007034302\n",
      "Batch 661. Accuracy=0.96875, Loss=0.11850747466087341\n",
      "Batch 671. Accuracy=1.0, Loss=0.03857162594795227\n",
      "Batch 681. Accuracy=0.96875, Loss=0.08702020347118378\n",
      "Batch 691. Accuracy=0.9375, Loss=0.11310525238513947\n",
      "Batch 701. Accuracy=0.84375, Loss=0.3371415436267853\n",
      "Batch 711. Accuracy=0.875, Loss=0.2849733829498291\n",
      "Batch 721. Accuracy=0.90625, Loss=0.6918730735778809\n",
      "Batch 731. Accuracy=0.96875, Loss=0.07902409136295319\n",
      "Batch 741. Accuracy=0.8125, Loss=0.6049695611000061\n",
      "Batch 751. Accuracy=0.90625, Loss=0.2832711637020111\n",
      "Batch 761. Accuracy=1.0, Loss=0.0663994699716568\n",
      "Batch 771. Accuracy=1.0, Loss=0.08935275673866272\n",
      "Batch 781. Accuracy=0.96875, Loss=0.07391603291034698\n",
      "Batch 791. Accuracy=0.9375, Loss=0.18092240393161774\n",
      "Batch 801. Accuracy=0.875, Loss=0.23412002623081207\n",
      "Batch 811. Accuracy=1.0, Loss=0.047962162643671036\n",
      "Batch 821. Accuracy=0.875, Loss=0.2787429094314575\n",
      "Batch 831. Accuracy=0.96875, Loss=0.089280866086483\n",
      "Batch 841. Accuracy=0.9375, Loss=0.2344302088022232\n",
      "Batch 851. Accuracy=0.875, Loss=0.2515881657600403\n",
      "Batch 861. Accuracy=1.0, Loss=0.042050641030073166\n",
      "Batch 871. Accuracy=0.96875, Loss=0.08161554485559464\n",
      "Batch 881. Accuracy=0.90625, Loss=0.6792548894882202\n",
      "Batch 891. Accuracy=0.78125, Loss=0.40026161074638367\n",
      "Batch 901. Accuracy=0.9375, Loss=0.08787529915571213\n",
      "Batch 911. Accuracy=0.96875, Loss=0.20688754320144653\n",
      "Batch 921. Accuracy=0.96875, Loss=0.10702197998762131\n",
      "Batch 931. Accuracy=0.9375, Loss=0.1072833389043808\n",
      "Batch 941. Accuracy=0.90625, Loss=0.18277588486671448\n",
      "Batch 951. Accuracy=0.875, Loss=0.5897988677024841\n",
      "Batch 961. Accuracy=1.0, Loss=0.007784230634570122\n",
      "Batch 971. Accuracy=0.9375, Loss=0.14308135211467743\n",
      "Batch 981. Accuracy=0.90625, Loss=0.18842823803424835\n",
      "Batch 991. Accuracy=0.9375, Loss=0.09893777221441269\n",
      "Batch 1001. Accuracy=0.96875, Loss=0.2274537831544876\n",
      "Batch 1011. Accuracy=0.90625, Loss=0.26289746165275574\n",
      "Batch 1021. Accuracy=0.9375, Loss=0.28977879881858826\n",
      "Batch 1031. Accuracy=0.84375, Loss=0.34100309014320374\n",
      "Batch 1041. Accuracy=1.0, Loss=0.02384953200817108\n",
      "Batch 1051. Accuracy=0.84375, Loss=0.25467854738235474\n",
      "Batch 1061. Accuracy=0.90625, Loss=0.30154767632484436\n",
      "Batch 1071. Accuracy=1.0, Loss=0.017930127680301666\n",
      "Batch 1081. Accuracy=0.9375, Loss=0.12221906334161758\n",
      "Batch 1091. Accuracy=0.96875, Loss=0.08701328188180923\n",
      "Batch 1101. Accuracy=0.96875, Loss=0.12842486798763275\n",
      "Batch 1111. Accuracy=0.875, Loss=0.31842735409736633\n",
      "Batch 1121. Accuracy=0.96875, Loss=0.2696288228034973\n",
      "Batch 1131. Accuracy=0.90625, Loss=0.236103817820549\n",
      "Batch 1141. Accuracy=0.9375, Loss=0.18855997920036316\n",
      "Batch 1151. Accuracy=0.96875, Loss=0.24481920897960663\n",
      "Batch 1161. Accuracy=0.90625, Loss=0.29813501238822937\n",
      "Batch 1171. Accuracy=0.9375, Loss=0.5308017730712891\n",
      "Epoch 1. acc: 0.92882 loss: 0.22396\n",
      "Batch 1. Accuracy=0.90625, Loss=0.19799891114234924\n",
      "Batch 11. Accuracy=0.96875, Loss=0.15083523094654083\n",
      "Batch 21. Accuracy=1.0, Loss=0.015174015425145626\n",
      "Batch 31. Accuracy=0.96875, Loss=0.3753044903278351\n",
      "Batch 41. Accuracy=0.9375, Loss=0.09491180628538132\n",
      "Batch 51. Accuracy=0.9375, Loss=0.08535349369049072\n",
      "Batch 61. Accuracy=0.90625, Loss=0.2553204298019409\n",
      "Batch 71. Accuracy=0.96875, Loss=0.044418204575777054\n",
      "Batch 81. Accuracy=0.90625, Loss=0.2510577440261841\n",
      "Batch 91. Accuracy=1.0, Loss=0.03633738309144974\n",
      "Batch 101. Accuracy=1.0, Loss=0.06684495508670807\n",
      "Batch 111. Accuracy=1.0, Loss=0.044439446181058884\n",
      "Batch 121. Accuracy=0.78125, Loss=0.3814481794834137\n",
      "Batch 131. Accuracy=0.96875, Loss=0.0988120511174202\n",
      "Batch 141. Accuracy=0.96875, Loss=0.051435306668281555\n",
      "Batch 151. Accuracy=0.9375, Loss=0.08328413963317871\n",
      "Batch 161. Accuracy=1.0, Loss=0.06771349161863327\n",
      "Batch 171. Accuracy=0.9375, Loss=0.26457998156547546\n",
      "Batch 181. Accuracy=0.9375, Loss=0.2215665578842163\n",
      "Batch 191. Accuracy=0.96875, Loss=0.10164311528205872\n",
      "Batch 201. Accuracy=0.9375, Loss=0.25145336985588074\n",
      "Batch 211. Accuracy=0.9375, Loss=0.1127382442355156\n",
      "Batch 221. Accuracy=0.96875, Loss=0.10330262780189514\n",
      "Batch 231. Accuracy=0.96875, Loss=0.08593802899122238\n",
      "Batch 241. Accuracy=1.0, Loss=0.0023222777526825666\n",
      "Batch 251. Accuracy=0.875, Loss=0.23771260678768158\n",
      "Batch 261. Accuracy=0.90625, Loss=0.22921906411647797\n",
      "Batch 271. Accuracy=0.875, Loss=0.5175657868385315\n",
      "Batch 281. Accuracy=0.90625, Loss=0.7090741991996765\n",
      "Batch 291. Accuracy=0.96875, Loss=0.031868793070316315\n",
      "Batch 301. Accuracy=0.90625, Loss=0.23459301888942719\n",
      "Batch 311. Accuracy=0.9375, Loss=0.17322607338428497\n",
      "Batch 321. Accuracy=0.96875, Loss=0.09196148067712784\n",
      "Batch 331. Accuracy=0.9375, Loss=0.13414837419986725\n",
      "Batch 341. Accuracy=0.96875, Loss=0.07850419729948044\n",
      "Batch 351. Accuracy=0.96875, Loss=0.19491887092590332\n",
      "Batch 361. Accuracy=0.875, Loss=0.374784916639328\n",
      "Batch 371. Accuracy=0.875, Loss=0.27464666962623596\n",
      "Batch 381. Accuracy=0.96875, Loss=0.06287790089845657\n",
      "Batch 391. Accuracy=0.90625, Loss=0.7564314603805542\n",
      "Batch 401. Accuracy=0.96875, Loss=0.13239657878875732\n",
      "Batch 411. Accuracy=0.90625, Loss=0.3499929904937744\n",
      "Batch 421. Accuracy=0.9375, Loss=0.2539491355419159\n",
      "Batch 431. Accuracy=0.90625, Loss=0.2028239518404007\n",
      "Batch 441. Accuracy=0.90625, Loss=0.22706961631774902\n",
      "Batch 451. Accuracy=0.96875, Loss=0.11767750233411789\n",
      "Batch 461. Accuracy=1.0, Loss=0.025011444464325905\n",
      "Batch 471. Accuracy=0.96875, Loss=0.07237505167722702\n",
      "Batch 481. Accuracy=0.90625, Loss=0.4811156392097473\n",
      "Batch 491. Accuracy=0.8125, Loss=0.5774040222167969\n",
      "Batch 501. Accuracy=0.9375, Loss=0.3665027618408203\n",
      "Batch 511. Accuracy=0.90625, Loss=0.24196405708789825\n",
      "Batch 521. Accuracy=0.9375, Loss=0.09879659861326218\n",
      "Batch 531. Accuracy=1.0, Loss=0.04554056376218796\n",
      "Batch 541. Accuracy=0.90625, Loss=0.47877076268196106\n",
      "Batch 551. Accuracy=0.9375, Loss=0.1104688048362732\n",
      "Batch 561. Accuracy=0.90625, Loss=0.3175542950630188\n",
      "Batch 571. Accuracy=0.875, Loss=0.3330274820327759\n",
      "Batch 581. Accuracy=0.96875, Loss=0.12329073250293732\n",
      "Batch 591. Accuracy=0.9375, Loss=0.12576329708099365\n",
      "Batch 601. Accuracy=0.9375, Loss=0.1219930425286293\n",
      "Batch 611. Accuracy=0.90625, Loss=0.2957055866718292\n",
      "Batch 621. Accuracy=0.9375, Loss=0.1136164665222168\n",
      "Batch 631. Accuracy=0.875, Loss=0.6756395101547241\n",
      "Batch 641. Accuracy=0.8125, Loss=0.32026153802871704\n",
      "Batch 651. Accuracy=0.90625, Loss=0.18499864637851715\n",
      "Batch 661. Accuracy=0.96875, Loss=0.09611418098211288\n",
      "Batch 671. Accuracy=0.96875, Loss=0.09216666221618652\n",
      "Batch 681. Accuracy=0.96875, Loss=0.10032910108566284\n",
      "Batch 691. Accuracy=0.96875, Loss=0.10287435352802277\n",
      "Batch 701. Accuracy=0.90625, Loss=0.15701021254062653\n",
      "Batch 711. Accuracy=0.9375, Loss=0.1784925013780594\n",
      "Batch 721. Accuracy=0.9375, Loss=0.12982362508773804\n",
      "Batch 731. Accuracy=0.875, Loss=0.8396331071853638\n",
      "Batch 741. Accuracy=0.84375, Loss=0.579484760761261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 751. Accuracy=0.84375, Loss=0.36383309960365295\n",
      "Batch 761. Accuracy=0.9375, Loss=0.1682513952255249\n",
      "Batch 771. Accuracy=0.9375, Loss=0.32951316237449646\n",
      "Batch 781. Accuracy=0.90625, Loss=0.15319421887397766\n",
      "Batch 791. Accuracy=0.90625, Loss=0.4933803975582123\n",
      "Batch 801. Accuracy=0.96875, Loss=0.12514157593250275\n",
      "Batch 811. Accuracy=0.90625, Loss=0.28702470660209656\n",
      "Batch 821. Accuracy=0.90625, Loss=0.31932714581489563\n",
      "Batch 831. Accuracy=0.90625, Loss=0.12134409695863724\n",
      "Batch 841. Accuracy=0.875, Loss=0.5898645520210266\n",
      "Batch 851. Accuracy=1.0, Loss=0.08306997269392014\n",
      "Batch 861. Accuracy=1.0, Loss=0.013880658894777298\n",
      "Batch 871. Accuracy=1.0, Loss=0.05010552331805229\n",
      "Batch 881. Accuracy=0.96875, Loss=0.09323794394731522\n",
      "Batch 891. Accuracy=0.96875, Loss=0.06133805587887764\n",
      "Batch 901. Accuracy=0.96875, Loss=0.024368425831198692\n",
      "Batch 911. Accuracy=0.96875, Loss=0.1599598377943039\n",
      "Batch 921. Accuracy=0.9375, Loss=0.10676269233226776\n",
      "Batch 931. Accuracy=1.0, Loss=0.015654990449547768\n",
      "Batch 941. Accuracy=0.84375, Loss=0.42478933930397034\n",
      "Batch 951. Accuracy=1.0, Loss=0.022781072184443474\n",
      "Batch 961. Accuracy=0.875, Loss=0.23300237953662872\n",
      "Batch 971. Accuracy=0.90625, Loss=0.17317022383213043\n",
      "Batch 981. Accuracy=0.875, Loss=0.45720434188842773\n",
      "Batch 991. Accuracy=0.96875, Loss=0.1001916155219078\n",
      "Batch 1001. Accuracy=0.875, Loss=0.15722611546516418\n",
      "Batch 1011. Accuracy=0.875, Loss=0.21766841411590576\n",
      "Batch 1021. Accuracy=0.9375, Loss=0.20803020894527435\n",
      "Batch 1031. Accuracy=0.96875, Loss=0.1557324230670929\n",
      "Batch 1041. Accuracy=0.9375, Loss=0.09040264785289764\n",
      "Batch 1051. Accuracy=0.96875, Loss=0.10242702811956406\n",
      "Batch 1061. Accuracy=0.9375, Loss=0.10236302763223648\n",
      "Batch 1071. Accuracy=0.9375, Loss=0.08708903193473816\n",
      "Batch 1081. Accuracy=0.96875, Loss=0.061584435403347015\n",
      "Batch 1091. Accuracy=0.90625, Loss=0.17249727249145508\n",
      "Batch 1101. Accuracy=0.90625, Loss=0.169388547539711\n",
      "Batch 1111. Accuracy=0.96875, Loss=0.08173823356628418\n",
      "Batch 1121. Accuracy=0.90625, Loss=0.21319220960140228\n",
      "Batch 1131. Accuracy=0.9375, Loss=0.127532497048378\n",
      "Batch 1141. Accuracy=0.96875, Loss=0.2685041129589081\n",
      "Batch 1151. Accuracy=0.96875, Loss=0.10871125012636185\n",
      "Batch 1161. Accuracy=1.0, Loss=0.01595495082437992\n",
      "Batch 1171. Accuracy=1.0, Loss=0.022816099226474762\n",
      "Epoch 2. acc: 0.93886 loss: 0.19549\n",
      "Batch 1. Accuracy=0.96875, Loss=0.04039299115538597\n",
      "Batch 11. Accuracy=0.96875, Loss=0.08128786087036133\n",
      "Batch 21. Accuracy=0.96875, Loss=0.06607101857662201\n",
      "Batch 31. Accuracy=0.9375, Loss=0.08024557679891586\n",
      "Batch 41. Accuracy=0.9375, Loss=0.142652690410614\n",
      "Batch 51. Accuracy=0.9375, Loss=0.25237756967544556\n",
      "Batch 61. Accuracy=1.0, Loss=0.06912138313055038\n",
      "Batch 71. Accuracy=0.9375, Loss=0.24481011927127838\n",
      "Batch 81. Accuracy=0.9375, Loss=0.13876326382160187\n",
      "Batch 91. Accuracy=0.90625, Loss=0.1423889547586441\n",
      "Batch 101. Accuracy=1.0, Loss=0.03812175244092941\n",
      "Batch 111. Accuracy=0.90625, Loss=0.19673223793506622\n",
      "Batch 121. Accuracy=0.875, Loss=0.1132190153002739\n",
      "Batch 131. Accuracy=0.9375, Loss=0.17731338739395142\n",
      "Batch 141. Accuracy=0.9375, Loss=0.4679150879383087\n",
      "Batch 151. Accuracy=0.90625, Loss=0.1321849524974823\n",
      "Batch 161. Accuracy=1.0, Loss=0.018449880182743073\n",
      "Batch 171. Accuracy=0.96875, Loss=0.05476125329732895\n",
      "Batch 181. Accuracy=0.90625, Loss=0.21470868587493896\n",
      "Batch 191. Accuracy=0.9375, Loss=0.23505373299121857\n",
      "Batch 201. Accuracy=0.90625, Loss=0.3153039813041687\n",
      "Batch 211. Accuracy=0.875, Loss=0.22905968129634857\n",
      "Batch 221. Accuracy=0.9375, Loss=0.13030625879764557\n",
      "Batch 231. Accuracy=0.875, Loss=0.7316391468048096\n",
      "Batch 241. Accuracy=0.90625, Loss=0.12186868488788605\n",
      "Batch 251. Accuracy=0.96875, Loss=0.06598708033561707\n",
      "Batch 261. Accuracy=0.9375, Loss=0.2620432376861572\n",
      "Batch 271. Accuracy=0.96875, Loss=0.11418744921684265\n",
      "Batch 281. Accuracy=0.8125, Loss=0.3553951680660248\n",
      "Batch 291. Accuracy=0.84375, Loss=0.5363395810127258\n",
      "Batch 301. Accuracy=0.9375, Loss=0.12495344877243042\n",
      "Batch 311. Accuracy=0.96875, Loss=0.09105649590492249\n",
      "Batch 321. Accuracy=0.9375, Loss=0.14463548362255096\n",
      "Batch 331. Accuracy=0.96875, Loss=0.19211958348751068\n",
      "Batch 341. Accuracy=0.90625, Loss=0.13257688283920288\n",
      "Batch 351. Accuracy=0.9375, Loss=0.2674947679042816\n",
      "Batch 361. Accuracy=0.90625, Loss=0.17814752459526062\n",
      "Batch 371. Accuracy=1.0, Loss=0.009612778201699257\n",
      "Batch 381. Accuracy=0.9375, Loss=0.2574602961540222\n",
      "Batch 391. Accuracy=0.9375, Loss=0.26944825053215027\n",
      "Batch 401. Accuracy=0.875, Loss=0.6234120726585388\n",
      "Batch 411. Accuracy=1.0, Loss=0.028392890468239784\n",
      "Batch 421. Accuracy=0.96875, Loss=0.15535232424736023\n",
      "Batch 431. Accuracy=0.9375, Loss=0.11404391378164291\n",
      "Batch 441. Accuracy=0.9375, Loss=0.10194959491491318\n",
      "Batch 451. Accuracy=0.96875, Loss=0.08735060691833496\n",
      "Batch 461. Accuracy=0.96875, Loss=0.08388837426900864\n",
      "Batch 471. Accuracy=0.875, Loss=0.37791144847869873\n",
      "Batch 481. Accuracy=1.0, Loss=0.03581470623612404\n",
      "Batch 491. Accuracy=0.96875, Loss=0.0769357681274414\n",
      "Batch 501. Accuracy=0.96875, Loss=0.15714915096759796\n",
      "Batch 511. Accuracy=0.90625, Loss=0.19292643666267395\n",
      "Batch 521. Accuracy=0.875, Loss=0.2630923092365265\n",
      "Batch 531. Accuracy=0.96875, Loss=0.06487319618463516\n",
      "Batch 541. Accuracy=0.9375, Loss=0.14365223050117493\n",
      "Batch 551. Accuracy=0.96875, Loss=0.0606817901134491\n",
      "Batch 561. Accuracy=0.9375, Loss=0.10600154101848602\n",
      "Batch 571. Accuracy=0.90625, Loss=0.29659759998321533\n",
      "Batch 581. Accuracy=0.9375, Loss=0.3544940948486328\n",
      "Batch 591. Accuracy=0.84375, Loss=0.4921416640281677\n",
      "Batch 601. Accuracy=0.9375, Loss=0.2783688008785248\n",
      "Batch 611. Accuracy=0.96875, Loss=0.1800832599401474\n",
      "Batch 621. Accuracy=0.90625, Loss=0.2922210991382599\n",
      "Batch 631. Accuracy=1.0, Loss=0.009367226622998714\n",
      "Batch 641. Accuracy=0.84375, Loss=0.3560752868652344\n",
      "Batch 651. Accuracy=0.9375, Loss=0.14587369561195374\n",
      "Batch 661. Accuracy=0.90625, Loss=0.5105299353599548\n",
      "Batch 671. Accuracy=0.96875, Loss=0.061448875814676285\n",
      "Batch 681. Accuracy=1.0, Loss=0.027469171211123466\n",
      "Batch 691. Accuracy=0.9375, Loss=0.2916128635406494\n",
      "Batch 701. Accuracy=0.9375, Loss=0.2753625214099884\n",
      "Batch 711. Accuracy=0.9375, Loss=0.13841447234153748\n",
      "Batch 721. Accuracy=0.96875, Loss=0.08720521628856659\n",
      "Batch 731. Accuracy=1.0, Loss=0.030886288732290268\n",
      "Batch 741. Accuracy=0.90625, Loss=0.5469709634780884\n",
      "Batch 751. Accuracy=0.96875, Loss=0.08150959759950638\n",
      "Batch 761. Accuracy=0.9375, Loss=0.06176941841840744\n",
      "Batch 771. Accuracy=1.0, Loss=0.02417837083339691\n",
      "Batch 781. Accuracy=0.875, Loss=0.5152263641357422\n",
      "Batch 791. Accuracy=0.9375, Loss=0.12963159382343292\n",
      "Batch 801. Accuracy=0.9375, Loss=0.7204753756523132\n",
      "Batch 811. Accuracy=0.9375, Loss=0.27616292238235474\n",
      "Batch 821. Accuracy=0.84375, Loss=0.26302969455718994\n",
      "Batch 831. Accuracy=0.9375, Loss=0.1801099181175232\n",
      "Batch 841. Accuracy=0.875, Loss=0.251379132270813\n",
      "Batch 851. Accuracy=0.9375, Loss=0.1718343198299408\n",
      "Batch 861. Accuracy=0.96875, Loss=0.06577283889055252\n",
      "Batch 871. Accuracy=1.0, Loss=0.016205914318561554\n",
      "Batch 881. Accuracy=0.875, Loss=0.4093005955219269\n",
      "Batch 891. Accuracy=1.0, Loss=0.014949532225728035\n",
      "Batch 901. Accuracy=1.0, Loss=0.039212264120578766\n",
      "Batch 911. Accuracy=0.96875, Loss=0.04406853765249252\n",
      "Batch 921. Accuracy=0.96875, Loss=0.04118318855762482\n",
      "Batch 931. Accuracy=1.0, Loss=0.0481504388153553\n",
      "Batch 941. Accuracy=1.0, Loss=0.02165035530924797\n",
      "Batch 951. Accuracy=0.9375, Loss=0.18369700014591217\n",
      "Batch 961. Accuracy=0.90625, Loss=0.17236623167991638\n",
      "Batch 971. Accuracy=0.90625, Loss=0.3752310574054718\n",
      "Batch 981. Accuracy=0.9375, Loss=0.08309798687696457\n",
      "Batch 991. Accuracy=0.96875, Loss=0.2883419990539551\n",
      "Batch 1001. Accuracy=0.96875, Loss=0.09731296449899673\n",
      "Batch 1011. Accuracy=0.90625, Loss=0.15854613482952118\n",
      "Batch 1021. Accuracy=0.9375, Loss=0.1245688870549202\n",
      "Batch 1031. Accuracy=0.875, Loss=0.31248369812965393\n",
      "Batch 1041. Accuracy=0.90625, Loss=0.2232508659362793\n",
      "Batch 1051. Accuracy=1.0, Loss=0.05553016439080238\n",
      "Batch 1061. Accuracy=0.9375, Loss=0.22989167273044586\n",
      "Batch 1071. Accuracy=0.9375, Loss=0.1948280930519104\n",
      "Batch 1081. Accuracy=1.0, Loss=0.010996589437127113\n",
      "Batch 1091. Accuracy=0.9375, Loss=0.10633628815412521\n",
      "Batch 1101. Accuracy=0.96875, Loss=0.03669470176100731\n",
      "Batch 1111. Accuracy=0.9375, Loss=0.20018252730369568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1121. Accuracy=0.96875, Loss=0.08099852502346039\n",
      "Batch 1131. Accuracy=0.9375, Loss=0.12007370591163635\n",
      "Batch 1141. Accuracy=0.90625, Loss=0.1264885663986206\n",
      "Batch 1151. Accuracy=1.0, Loss=0.0424153208732605\n",
      "Batch 1161. Accuracy=0.90625, Loss=0.1301383227109909\n",
      "Batch 1171. Accuracy=0.96875, Loss=0.047750312834978104\n",
      "Epoch 3. acc: 0.94208 loss: 0.18906\n",
      "Batch 1. Accuracy=1.0, Loss=0.05123085901141167\n",
      "Batch 11. Accuracy=0.96875, Loss=0.10530781000852585\n",
      "Batch 21. Accuracy=0.90625, Loss=0.12931247055530548\n",
      "Batch 31. Accuracy=0.9375, Loss=0.23939409852027893\n",
      "Batch 41. Accuracy=0.96875, Loss=0.0854935497045517\n",
      "Batch 51. Accuracy=0.96875, Loss=0.060197614133358\n",
      "Batch 61. Accuracy=0.96875, Loss=0.09009959548711777\n",
      "Batch 71. Accuracy=0.9375, Loss=0.08540253341197968\n",
      "Batch 81. Accuracy=0.9375, Loss=0.14173181354999542\n",
      "Batch 91. Accuracy=0.96875, Loss=0.04846572503447533\n",
      "Batch 101. Accuracy=0.96875, Loss=0.22440248727798462\n",
      "Batch 111. Accuracy=0.90625, Loss=0.2047707587480545\n",
      "Batch 121. Accuracy=0.9375, Loss=0.11801756173372269\n",
      "Batch 131. Accuracy=0.9375, Loss=0.2451934814453125\n",
      "Batch 141. Accuracy=0.9375, Loss=0.20180216431617737\n",
      "Batch 151. Accuracy=0.90625, Loss=1.03731107711792\n",
      "Batch 161. Accuracy=0.8125, Loss=0.7448320984840393\n",
      "Batch 171. Accuracy=0.9375, Loss=0.33787497878074646\n",
      "Batch 181. Accuracy=0.90625, Loss=0.4004601836204529\n",
      "Batch 191. Accuracy=0.96875, Loss=0.0908999890089035\n",
      "Batch 201. Accuracy=0.90625, Loss=0.19560624659061432\n",
      "Batch 211. Accuracy=0.9375, Loss=0.16614751517772675\n",
      "Batch 221. Accuracy=0.9375, Loss=0.27797242999076843\n",
      "Batch 231. Accuracy=0.9375, Loss=0.2361525297164917\n",
      "Batch 241. Accuracy=0.90625, Loss=0.27039235830307007\n",
      "Batch 251. Accuracy=0.90625, Loss=0.15053430199623108\n",
      "Batch 261. Accuracy=0.9375, Loss=0.30404332280158997\n",
      "Batch 271. Accuracy=1.0, Loss=0.03580654785037041\n",
      "Batch 281. Accuracy=0.9375, Loss=0.17942912876605988\n",
      "Batch 291. Accuracy=0.9375, Loss=0.28083571791648865\n",
      "Batch 301. Accuracy=0.90625, Loss=0.23102498054504395\n",
      "Batch 311. Accuracy=0.90625, Loss=0.46952584385871887\n",
      "Batch 321. Accuracy=0.875, Loss=0.1836509108543396\n",
      "Batch 331. Accuracy=0.96875, Loss=0.14784419536590576\n",
      "Batch 341. Accuracy=1.0, Loss=0.04669220373034477\n",
      "Batch 351. Accuracy=0.9375, Loss=0.10756716877222061\n",
      "Batch 361. Accuracy=0.9375, Loss=0.3632916212081909\n",
      "Batch 371. Accuracy=0.875, Loss=0.6677911281585693\n",
      "Batch 381. Accuracy=0.875, Loss=0.18368038535118103\n",
      "Batch 391. Accuracy=0.875, Loss=0.36216920614242554\n",
      "Batch 401. Accuracy=0.96875, Loss=0.10541369765996933\n",
      "Batch 411. Accuracy=0.875, Loss=0.15638437867164612\n",
      "Batch 421. Accuracy=1.0, Loss=0.08007576316595078\n",
      "Batch 431. Accuracy=0.84375, Loss=0.36950281262397766\n",
      "Batch 441. Accuracy=0.9375, Loss=0.21511630713939667\n",
      "Batch 451. Accuracy=0.9375, Loss=0.15536150336265564\n",
      "Batch 461. Accuracy=0.90625, Loss=0.2613019347190857\n",
      "Batch 471. Accuracy=0.9375, Loss=0.341647207736969\n",
      "Batch 481. Accuracy=0.96875, Loss=0.060878586024045944\n",
      "Batch 491. Accuracy=0.9375, Loss=0.1528012752532959\n",
      "Batch 501. Accuracy=0.875, Loss=0.20968028903007507\n",
      "Batch 511. Accuracy=0.875, Loss=0.2531523108482361\n",
      "Batch 521. Accuracy=0.875, Loss=0.4312790632247925\n",
      "Batch 531. Accuracy=0.90625, Loss=0.4131884276866913\n",
      "Batch 541. Accuracy=1.0, Loss=0.023815488442778587\n",
      "Batch 551. Accuracy=1.0, Loss=0.023554429411888123\n",
      "Batch 561. Accuracy=0.875, Loss=0.3693917393684387\n",
      "Batch 571. Accuracy=0.90625, Loss=0.4536026120185852\n",
      "Batch 581. Accuracy=0.96875, Loss=0.07830844819545746\n",
      "Batch 591. Accuracy=0.90625, Loss=0.44740861654281616\n",
      "Batch 601. Accuracy=0.9375, Loss=0.12613382935523987\n",
      "Batch 611. Accuracy=0.9375, Loss=0.13673828542232513\n",
      "Batch 621. Accuracy=0.875, Loss=0.39564049243927\n",
      "Batch 631. Accuracy=1.0, Loss=0.031502507627010345\n",
      "Batch 641. Accuracy=0.9375, Loss=0.22426684200763702\n",
      "Batch 651. Accuracy=0.9375, Loss=0.44477930665016174\n",
      "Batch 661. Accuracy=0.9375, Loss=0.09268879890441895\n",
      "Batch 671. Accuracy=0.96875, Loss=0.06619547307491302\n",
      "Batch 681. Accuracy=0.96875, Loss=0.07573995739221573\n",
      "Batch 691. Accuracy=1.0, Loss=0.021090125665068626\n",
      "Batch 701. Accuracy=0.96875, Loss=0.15557783842086792\n",
      "Batch 711. Accuracy=0.96875, Loss=0.14517712593078613\n",
      "Batch 721. Accuracy=0.9375, Loss=0.12658169865608215\n",
      "Batch 731. Accuracy=0.96875, Loss=0.06605023890733719\n",
      "Batch 741. Accuracy=1.0, Loss=0.006842356640845537\n",
      "Batch 751. Accuracy=0.9375, Loss=0.2812051773071289\n",
      "Batch 761. Accuracy=0.9375, Loss=0.07729525864124298\n",
      "Batch 771. Accuracy=0.90625, Loss=0.29406067728996277\n",
      "Batch 781. Accuracy=0.96875, Loss=0.19673959910869598\n",
      "Batch 791. Accuracy=1.0, Loss=0.025824513286352158\n",
      "Batch 801. Accuracy=0.96875, Loss=0.15875980257987976\n",
      "Batch 811. Accuracy=0.9375, Loss=0.25477123260498047\n",
      "Batch 821. Accuracy=0.84375, Loss=0.2267993837594986\n",
      "Batch 831. Accuracy=1.0, Loss=0.016670333221554756\n",
      "Batch 841. Accuracy=0.96875, Loss=0.18663880228996277\n",
      "Batch 851. Accuracy=0.96875, Loss=0.2195722758769989\n",
      "Batch 861. Accuracy=1.0, Loss=0.14000101387500763\n",
      "Batch 871. Accuracy=0.9375, Loss=0.08265562355518341\n",
      "Batch 881. Accuracy=0.9375, Loss=0.18586406111717224\n",
      "Batch 891. Accuracy=0.90625, Loss=0.22771984338760376\n",
      "Batch 901. Accuracy=0.875, Loss=0.37480807304382324\n",
      "Batch 911. Accuracy=0.875, Loss=0.8722423315048218\n",
      "Batch 921. Accuracy=0.96875, Loss=0.0951927974820137\n",
      "Batch 931. Accuracy=0.9375, Loss=0.07262589782476425\n",
      "Batch 941. Accuracy=0.96875, Loss=0.11932677030563354\n",
      "Batch 951. Accuracy=0.9375, Loss=0.1428063064813614\n",
      "Batch 961. Accuracy=1.0, Loss=0.03063582256436348\n",
      "Batch 971. Accuracy=0.96875, Loss=0.025763975456357002\n",
      "Batch 981. Accuracy=0.9375, Loss=0.08913566917181015\n",
      "Batch 991. Accuracy=0.96875, Loss=0.053778525441884995\n",
      "Batch 1001. Accuracy=0.96875, Loss=0.08954643458127975\n",
      "Batch 1011. Accuracy=0.9375, Loss=0.2967357039451599\n",
      "Batch 1021. Accuracy=0.96875, Loss=0.20633050799369812\n",
      "Batch 1031. Accuracy=0.9375, Loss=0.0860847681760788\n",
      "Batch 1041. Accuracy=0.875, Loss=0.508353054523468\n",
      "Batch 1051. Accuracy=0.9375, Loss=0.09515996277332306\n",
      "Batch 1061. Accuracy=0.9375, Loss=0.2694043815135956\n",
      "Batch 1071. Accuracy=0.90625, Loss=0.9688579440116882\n",
      "Batch 1081. Accuracy=0.96875, Loss=0.06653302162885666\n",
      "Batch 1091. Accuracy=0.96875, Loss=0.10024849325418472\n",
      "Batch 1101. Accuracy=0.9375, Loss=0.13827858865261078\n",
      "Batch 1111. Accuracy=0.90625, Loss=0.17989367246627808\n",
      "Batch 1121. Accuracy=0.9375, Loss=0.17709986865520477\n",
      "Batch 1131. Accuracy=0.9375, Loss=0.33801978826522827\n",
      "Batch 1141. Accuracy=0.9375, Loss=0.1726497858762741\n",
      "Batch 1151. Accuracy=0.875, Loss=0.41215816140174866\n",
      "Batch 1161. Accuracy=0.9375, Loss=0.2313598245382309\n",
      "Batch 1171. Accuracy=0.90625, Loss=0.24029432237148285\n",
      "Epoch 4. acc: 0.94272 loss: 0.19187\n"
     ]
    }
   ],
   "source": [
    "train_hist, valid_hist = train(vgg_model, epochs=5, optimizer=optimizer_vgg, loss_func=loss_function_vgg, train_loader=vgg_dl_train, valid_loader=None, train_frac=0.5) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_hist[0], color='green', label='accuracy')\n",
    "plt.plot(train_hist[1], color='orange', label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.9262147570485902\n"
     ]
    }
   ],
   "source": [
    "preds, real = validate_model(vgg_model, vgg_dl_valid, num_samples=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 428, 3, 3]) 3852\n",
      "Batch 1. Accuracy=0.53125, Loss=0.6928560733795166\n",
      "Batch 101. Accuracy=0.34375, Loss=0.7169457077980042\n",
      "Batch 201. Accuracy=0.5, Loss=0.692734956741333\n",
      "Batch 301. Accuracy=0.65625, Loss=0.6896031498908997\n",
      "Batch 401. Accuracy=0.59375, Loss=0.6610868573188782\n",
      "Batch 501. Accuracy=0.625, Loss=0.6847348809242249\n",
      "Batch 601. Accuracy=0.65625, Loss=0.6011820435523987\n",
      "Batch 701. Accuracy=0.71875, Loss=0.5437538027763367\n",
      "Epoch 0. acc: 0.62793 loss: 0.62057\n",
      "Batch 1. Accuracy=0.78125, Loss=0.5079799890518188\n",
      "Batch 101. Accuracy=0.875, Loss=0.2762523889541626\n",
      "Batch 201. Accuracy=0.84375, Loss=0.37937843799591064\n",
      "Batch 301. Accuracy=0.75, Loss=0.5214934945106506\n",
      "Batch 401. Accuracy=0.8125, Loss=0.35650864243507385\n",
      "Batch 501. Accuracy=0.84375, Loss=0.39418888092041016\n",
      "Batch 601. Accuracy=0.875, Loss=0.34674209356307983\n",
      "Batch 701. Accuracy=0.90625, Loss=0.263385146856308\n",
      "Epoch 1. acc: 0.84251 loss: 0.37466\n",
      "Batch 1. Accuracy=0.84375, Loss=0.252165287733078\n",
      "Batch 101. Accuracy=0.90625, Loss=0.24098803102970123\n",
      "Batch 201. Accuracy=0.9375, Loss=0.19240504503250122\n",
      "Batch 301. Accuracy=0.90625, Loss=0.27373412251472473\n",
      "Batch 401. Accuracy=0.84375, Loss=0.3254263997077942\n",
      "Batch 501. Accuracy=0.75, Loss=0.7283262014389038\n",
      "Batch 601. Accuracy=0.96875, Loss=0.09132951498031616\n",
      "Batch 701. Accuracy=0.9375, Loss=0.21605093777179718\n",
      "Epoch 2. acc: 0.89224 loss: 0.27249\n",
      "Batch 1. Accuracy=0.84375, Loss=0.29596415162086487\n",
      "Batch 101. Accuracy=0.90625, Loss=0.16651688516139984\n",
      "Batch 201. Accuracy=0.90625, Loss=0.2418825328350067\n",
      "Batch 301. Accuracy=0.96875, Loss=0.14473329484462738\n",
      "Batch 401. Accuracy=0.9375, Loss=0.14776183664798737\n",
      "Batch 501. Accuracy=0.9375, Loss=0.15358565747737885\n",
      "Batch 601. Accuracy=0.9375, Loss=0.11163555830717087\n",
      "Batch 701. Accuracy=0.90625, Loss=0.22660160064697266\n",
      "Epoch 3. acc: 0.91241 loss: 0.21706\n",
      "Batch 1. Accuracy=0.96875, Loss=0.13922971487045288\n",
      "Batch 101. Accuracy=0.9375, Loss=0.23487402498722076\n",
      "Batch 201. Accuracy=0.90625, Loss=0.15034544467926025\n",
      "Batch 301. Accuracy=0.84375, Loss=0.2590484321117401\n",
      "Batch 401. Accuracy=0.96875, Loss=0.07579311728477478\n",
      "Batch 501. Accuracy=0.84375, Loss=0.5269393920898438\n",
      "Batch 601. Accuracy=0.9375, Loss=0.11370611935853958\n",
      "Batch 701. Accuracy=0.96875, Loss=0.061861030757427216\n",
      "Epoch 4. acc: 0.92456 loss: 0.19347\n",
      "Batch 1. Accuracy=0.9375, Loss=0.28279227018356323\n",
      "Batch 101. Accuracy=1.0, Loss=0.06030702590942383\n",
      "Batch 201. Accuracy=0.96875, Loss=0.05578025430440903\n",
      "Batch 301. Accuracy=0.96875, Loss=0.18005028367042542\n",
      "Batch 401. Accuracy=0.9375, Loss=0.10978385806083679\n",
      "Batch 501. Accuracy=0.90625, Loss=0.18812452256679535\n",
      "Batch 601. Accuracy=0.9375, Loss=0.1896153837442398\n",
      "Batch 701. Accuracy=1.0, Loss=0.05358448624610901\n",
      "Epoch 5. acc: 0.93067 loss: 0.17204\n",
      "Batch 1. Accuracy=0.9375, Loss=0.19614407420158386\n",
      "Batch 101. Accuracy=0.90625, Loss=0.23574545979499817\n",
      "Batch 201. Accuracy=1.0, Loss=0.09828294068574905\n",
      "Batch 301. Accuracy=0.96875, Loss=0.13422904908657074\n",
      "Batch 401. Accuracy=1.0, Loss=0.03567942604422569\n",
      "Batch 501. Accuracy=0.9375, Loss=0.19919045269489288\n",
      "Batch 601. Accuracy=0.96875, Loss=0.059689197689294815\n",
      "Batch 701. Accuracy=0.90625, Loss=0.4255416691303253\n",
      "Epoch 6. acc: 0.93666 loss: 0.1632\n",
      "Batch 1. Accuracy=1.0, Loss=0.03531437739729881\n",
      "Batch 101. Accuracy=0.875, Loss=0.1993950605392456\n",
      "Batch 201. Accuracy=0.90625, Loss=0.21492157876491547\n",
      "Batch 301. Accuracy=0.96875, Loss=0.11012148857116699\n",
      "Batch 401. Accuracy=0.9375, Loss=0.08097261935472488\n",
      "Batch 501. Accuracy=0.96875, Loss=0.11667212843894958\n",
      "Batch 601. Accuracy=0.875, Loss=0.3987807333469391\n",
      "Batch 701. Accuracy=0.90625, Loss=0.27969321608543396\n",
      "Epoch 7. acc: 0.94109 loss: 0.15017\n",
      "Batch 1. Accuracy=0.9375, Loss=0.17932458221912384\n",
      "Batch 101. Accuracy=1.0, Loss=0.03479042649269104\n",
      "Batch 201. Accuracy=0.96875, Loss=0.10307666659355164\n",
      "Batch 301. Accuracy=0.84375, Loss=0.28394001722335815\n",
      "Batch 401. Accuracy=0.9375, Loss=0.24288251996040344\n",
      "Batch 501. Accuracy=0.9375, Loss=0.14558257162570953\n",
      "Batch 601. Accuracy=0.9375, Loss=0.18387828767299652\n",
      "Batch 701. Accuracy=0.90625, Loss=0.2403145134449005\n",
      "Epoch 8. acc: 0.94473 loss: 0.14424\n",
      "Batch 1. Accuracy=0.90625, Loss=0.32162848114967346\n",
      "Batch 101. Accuracy=0.84375, Loss=0.22098690271377563\n",
      "Batch 201. Accuracy=0.96875, Loss=0.12047579884529114\n",
      "Batch 301. Accuracy=0.96875, Loss=0.15106931328773499\n",
      "Batch 401. Accuracy=0.9375, Loss=0.11838524788618088\n",
      "Batch 501. Accuracy=0.9375, Loss=0.14923322200775146\n",
      "Batch 601. Accuracy=0.96875, Loss=0.058370500802993774\n",
      "Batch 701. Accuracy=1.0, Loss=0.04935822635889053\n",
      "Epoch 9. acc: 0.94858 loss: 0.13675\n"
     ]
    }
   ],
   "source": [
    "train_hist_cnn, valid_hist_cnn = train(model, epochs=10, optimizer=optimizer, loss_func=loss_function, train_loader=tensor_dl_train, valid_loader=None, train_frac=0.3) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_hist_cnn[0], color='green', label='accuracy')\n",
    "plt.plot(train_hist_cnn[1], color='orange', label='loss')\n",
    "plt.title(\"Simple CNN model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg_model, 'vgg_model_trained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg_model, 'my_model_trained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env]",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
